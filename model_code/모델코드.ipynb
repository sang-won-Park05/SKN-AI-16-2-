{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!apt-get update -qq\n",
        "!apt-get install fonts-nanum -qq\n",
        "!fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "!pip install minisom\n",
        "!pip install catboost\n",
        "!pip install xgboost lightgbm catboost minisom tensorflow\n",
        "!pip install optuna minisom catboost\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fontprop = fm.FontProperties(fname=font_path, size=10)\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjPdhIWGxvSn",
        "outputId": "9e5fed3d-53dd-4b06-f089-584a8e1ea996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "df = pd.read_csv('/content/Political_Party_Churn_Data.csv')\n",
        "df = df.drop(['MemberID'], axis=1)\n",
        "for col in df.select_dtypes(include='object'):\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_train_arr, X_test_arr = X_train, X_test\n",
        "results = {}\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import mode\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Reshape\n",
        "from minisom import MiniSom\n",
        "\n",
        "def cluster_to_label(y_true, cluster_pred):\n",
        "    labels = np.zeros_like(cluster_pred)\n",
        "    for i in range(len(np.unique(cluster_pred))):\n",
        "        mask = (cluster_pred == i)\n",
        "        if np.sum(mask) > 0:\n",
        "            labels[mask] = mode(y_true[mask], keepdims=True)[0]\n",
        "    return labels\n",
        "\n",
        "# ---- 1. Linear Models ----\n",
        "def objective_logreg(trial):\n",
        "    c = trial.suggest_float('C', 0.001, 10, log=True)\n",
        "    solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear'])\n",
        "    model = LogisticRegression(max_iter=500, C=c, solver=solver)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_logreg = optuna.create_study(direction='maximize')\n",
        "study_logreg.optimize(objective_logreg, n_trials=3)\n",
        "results['LogisticRegression'] = study_logreg.best_value\n",
        "\n",
        "def objective_ridge(trial):\n",
        "    alpha = trial.suggest_float('alpha', 0.01, 10, log=True)\n",
        "    model = RidgeClassifier(alpha=alpha)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_ridge = optuna.create_study(direction='maximize')\n",
        "study_ridge.optimize(objective_ridge, n_trials=3)\n",
        "results['RidgeClassifier'] = study_ridge.best_value\n",
        "\n",
        "def objective_sgd(trial):\n",
        "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
        "    loss = trial.suggest_categorical('loss', ['hinge', 'log_loss'])\n",
        "    model = SGDClassifier(max_iter=1000, tol=1e-3, alpha=alpha, loss=loss)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_sgd = optuna.create_study(direction='maximize')\n",
        "study_sgd.optimize(objective_sgd, n_trials=3)\n",
        "results['SGDClassifier'] = study_sgd.best_value\n",
        "\n",
        "def objective_perceptron(trial):\n",
        "    eta0 = trial.suggest_float('eta0', 1e-4, 1.0, log=True)\n",
        "    model = Perceptron(max_iter=1000, eta0=eta0)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_perceptron = optuna.create_study(direction='maximize')\n",
        "study_perceptron.optimize(objective_perceptron, n_trials=3)\n",
        "results['Perceptron'] = study_perceptron.best_value\n",
        "\n",
        "# ---- 2. Decision Tree ----\n",
        "def objective_tree(trial):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 16),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 8),\n",
        "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "    }\n",
        "    model = DecisionTreeClassifier(**params, random_state=42)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_tree = optuna.create_study(direction=\"maximize\")\n",
        "study_tree.optimize(objective_tree, n_trials=3)\n",
        "results['DecisionTree'] = study_tree.best_value\n",
        "\n",
        "# ---- 3. Ensemble ----\n",
        "def objective_rf(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 6),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4)\n",
        "    }\n",
        "    rf = RandomForestClassifier(**params, random_state=42)\n",
        "    rf.fit(X_train_arr, y_train)\n",
        "    preds = rf.predict(X_test_arr)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    return acc\n",
        "study_rf = optuna.create_study(direction=\"maximize\")\n",
        "study_rf.optimize(objective_rf, n_trials=3)\n",
        "results['RandomForest'] = study_rf.best_value\n",
        "\n",
        "def objective_gb(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2)\n",
        "    }\n",
        "    model = GradientBoostingClassifier(**params)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_gb = optuna.create_study(direction='maximize')\n",
        "study_gb.optimize(objective_gb, n_trials=3)\n",
        "results['GradientBoosting'] = study_gb.best_value\n",
        "\n",
        "def objective_ada(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0)\n",
        "    }\n",
        "    model = AdaBoostClassifier(**params)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_ada = optuna.create_study(direction='maximize')\n",
        "study_ada.optimize(objective_ada, n_trials=3)\n",
        "results['AdaBoost'] = study_ada.best_value\n",
        "\n",
        "# ---- 4. SVC ----\n",
        "def objective_svc(trial):\n",
        "    c = trial.suggest_float('C', 0.01, 10, log=True)\n",
        "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
        "    model = SVC(C=c, kernel=kernel, probability=True)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    return accuracy_score(y_test, model.predict(X_test_arr))\n",
        "study_svc = optuna.create_study(direction='maximize')\n",
        "study_svc.optimize(objective_svc, n_trials=3)\n",
        "results['SVC'] = study_svc.best_value\n",
        "\n",
        "# ---- 5. KMeans ----\n",
        "def objective_kmeans(trial):\n",
        "    n_clusters = trial.suggest_int('n_clusters', 2, 4)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X_train_arr)\n",
        "    cluster_pred = kmeans.predict(X_test_arr)\n",
        "    pred_labels = cluster_to_label(y_test.values, cluster_pred)\n",
        "    return accuracy_score(y_test, pred_labels)\n",
        "study_kmeans = optuna.create_study(direction='maximize')\n",
        "study_kmeans.optimize(objective_kmeans, n_trials=3)\n",
        "results['KMeans'] = study_kmeans.best_value\n",
        "\n",
        "# ---- 6. Boosting ----\n",
        "def objective_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'use_label_encoder': False,\n",
        "        'eval_metric': 'logloss',\n",
        "        'verbosity': 0\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    pred = model.predict(X_test_arr)\n",
        "    return accuracy_score(y_test, pred)\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, n_trials=3)\n",
        "results['XGBoost'] = study_xgb.best_value\n",
        "\n",
        "def objective_lgbm(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 10, 40)\n",
        "    }\n",
        "    model = LGBMClassifier(**params)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    pred = model.predict(X_test_arr)\n",
        "    return accuracy_score(y_test, pred)\n",
        "study_lgbm = optuna.create_study(direction='maximize')\n",
        "study_lgbm.optimize(objective_lgbm, n_trials=3)\n",
        "results['LightGBM'] = study_lgbm.best_value\n",
        "\n",
        "def objective_cat(trial):\n",
        "    params = {\n",
        "        'iterations': trial.suggest_int('iterations', 50, 100),\n",
        "        'depth': trial.suggest_int('depth', 2, 8),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "    }\n",
        "    model = CatBoostClassifier(**params, verbose=0)\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    pred = model.predict(X_test_arr)\n",
        "    return accuracy_score(y_test, pred)\n",
        "study_cat = optuna.create_study(direction='maximize')\n",
        "study_cat.optimize(objective_cat, n_trials=3)\n",
        "results['CatBoost'] = study_cat.best_value\n",
        "\n",
        "# ---- 7. sklearn MLP ----\n",
        "def objective_mlp(trial):\n",
        "    hidden_layer_sizes = tuple([trial.suggest_int(f'n_units_{i}', 32, 64) for i in range(2)])\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=200)\n",
        "    mlp.fit(X_train_arr, y_train)\n",
        "    pred = mlp.predict(X_test_arr)\n",
        "    return accuracy_score(y_test, pred)\n",
        "study_mlp = optuna.create_study(direction='maximize')\n",
        "study_mlp.optimize(objective_mlp, n_trials=3)\n",
        "results['MLP_sklearn'] = study_mlp.best_value\n",
        "\n",
        "# ---- 8. Keras MLP, LSTM, RNN ----\n",
        "def objective_keras(trial, model_type):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    n1 = trial.suggest_int('n1', 32, 64)\n",
        "    n2 = trial.suggest_int('n2', 16, 32)\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    if model_type == 'MLP':\n",
        "        model.add(Dense(n1, activation='relu', input_shape=(X_train_arr.shape[1],)))\n",
        "        model.add(Dense(n2, activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    elif model_type == 'LSTM':\n",
        "        model.add(Reshape((X_train_arr.shape[1], 1), input_shape=(X_train_arr.shape[1],)))\n",
        "        model.add(LSTM(n1))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    elif model_type == 'RNN':\n",
        "        model.add(Reshape((X_train_arr.shape[1], 1), input_shape=(X_train_arr.shape[1],)))\n",
        "        model.add(SimpleRNN(n1))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='binary_crossentropy')\n",
        "    model.fit(X_train_arr, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "    pred = model.predict(X_test_arr)\n",
        "    pred_label = (pred.flatten() > 0.5).astype(int)\n",
        "    return accuracy_score(y_test, pred_label)\n",
        "\n",
        "study_keras_mlp = optuna.create_study(direction='maximize')\n",
        "study_keras_mlp.optimize(lambda trial: objective_keras(trial, 'MLP'), n_trials=3)\n",
        "results['MLP_Keras'] = study_keras_mlp.best_value\n",
        "\n",
        "study_keras_lstm = optuna.create_study(direction='maximize')\n",
        "study_keras_lstm.optimize(lambda trial: objective_keras(trial, 'LSTM'), n_trials=3)\n",
        "results['LSTM_Keras'] = study_keras_lstm.best_value\n",
        "\n",
        "study_keras_rnn = optuna.create_study(direction='maximize')\n",
        "study_keras_rnn.optimize(lambda trial: objective_keras(trial, 'RNN'), n_trials=3)\n",
        "results['RNN_Keras'] = study_keras_rnn.best_value\n",
        "\n",
        "# ---- 9. MiniSom ----\n",
        "def objective_som(trial):\n",
        "    x = trial.suggest_int('x', 1, 2)\n",
        "    y = trial.suggest_int('y', 2, 3)\n",
        "    sigma = trial.suggest_float('sigma', 0.1, 1.0)\n",
        "    lr = trial.suggest_float('learning_rate', 0.01, 1.0)\n",
        "    som = MiniSom(x=x, y=y, input_len=X_train_arr.shape[1], sigma=sigma, learning_rate=lr, random_seed=42)\n",
        "    som.train(X_train_arr, 200)\n",
        "    def som_predict(som, X):\n",
        "        pred = []\n",
        "        for xx in X:\n",
        "            win = som.winner(xx)\n",
        "            cluster = win[1]\n",
        "            pred.append(cluster)\n",
        "        return np.array(pred)\n",
        "    som_pred = som_predict(som, X_test_arr)\n",
        "    som_pred_labels = cluster_to_label(y_test.values, som_pred)\n",
        "    return accuracy_score(y_test, som_pred_labels)\n",
        "study_som = optuna.create_study(direction='maximize')\n",
        "study_som.optimize(objective_som, n_trials=3)\n",
        "results['MiniSom'] = study_som.best_value\n",
        "\n",
        "# --- 정확도 시각화 ---\n",
        "plt.figure(figsize=(11, 9))\n",
        "sorted_results = dict(sorted(results.items(), key=lambda x: x[1]))\n",
        "plt.barh(list(sorted_results.keys()), list(sorted_results.values()), color='skyblue')\n",
        "plt.xlabel('accuracy_score', fontsize=14)\n",
        "plt.title('Optuna 튜닝 모델별 Churn 예측 정확도', fontsize=16)\n",
        "plt.xlim(0,1)\n",
        "for i, (name, acc) in enumerate(sorted_results.items()):\n",
        "    plt.text(acc+0.01, i, f'{acc:.3f}', va='center', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-b44w82gRfPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "roc_auc_dict = {}\n",
        "\n",
        "def get_proba_or_score(model, X):\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model.predict_proba(X)[:,1]\n",
        "    elif hasattr(model, \"decision_function\"):\n",
        "        return model.decision_function(X)\n",
        "    else:\n",
        "        return model.predict(X)\n",
        "\n",
        "model_dict = {\n",
        "    \"LogisticRegression_Optuna\": LogisticRegression(**study_logreg.best_trial.params, max_iter=500),\n",
        "    \"RidgeClassifier_Optuna\": RidgeClassifier(**study_ridge.best_trial.params),\n",
        "    \"SGDClassifier_Optuna\": SGDClassifier(**study_sgd.best_trial.params),\n",
        "    \"Perceptron_Optuna\": Perceptron(**study_perceptron.best_trial.params, max_iter=1000),\n",
        "    \"DecisionTree_Optuna\": DecisionTreeClassifier(**study_tree.best_trial.params, random_state=42),\n",
        "    \"RandomForest_Optuna\": RandomForestClassifier(**study_rf.best_trial.params, random_state=42),\n",
        "    \"GradientBoosting_Optuna\": GradientBoostingClassifier(**study_gb.best_trial.params),\n",
        "    \"AdaBoost_Optuna\": AdaBoostClassifier(**study_ada.best_trial.params),\n",
        "    \"SVC_Optuna\": SVC(**study_svc.best_trial.params, probability=True),\n",
        "    \"XGBoost_Optuna\": XGBClassifier(**study_xgb.best_trial.params, use_label_encoder=False, eval_metric='logloss', verbosity=0),\n",
        "    \"LightGBM_Optuna\": LGBMClassifier(**study_lgbm.best_trial.params),\n",
        "    \"CatBoost_Optuna\": CatBoostClassifier(**study_cat.best_trial.params, verbose=0),\n",
        "}\n",
        "\n",
        "for name, model in model_dict.items():\n",
        "    model.fit(X_train_arr, y_train)\n",
        "    try:\n",
        "        y_score = get_proba_or_score(model, X_test_arr)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "        auc = roc_auc_score(y_test, y_score)\n",
        "        # _Optuna 및 _ → \" \" 로 변환\n",
        "        clean_label = name.replace(\"_Optuna\", \"\").replace(\"_\", \" \")\n",
        "        plt.plot(fpr, tpr, lw=2, label=f\"{clean_label} (AUC={auc:.3f})\")\n",
        "        roc_auc_dict[name] = auc\n",
        "    except Exception as e:\n",
        "        print(f\"ROC Error in {name}: {e}\")\n",
        "\n",
        "# KMeans\n",
        "try:\n",
        "    n_clusters = study_kmeans.best_trial.params['n_clusters']\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X_train_arr)\n",
        "    cluster_pred = kmeans.predict(X_test_arr)\n",
        "    kmeans_label = cluster_to_label(y_test.values, cluster_pred)\n",
        "    auc = roc_auc_score(y_test, kmeans_label)\n",
        "    fpr, tpr, _ = roc_curve(y_test, kmeans_label)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"KMeans (AUC={auc:.3f})\")   # 수정\n",
        "    roc_auc_dict[\"KMeans_Optuna\"] = auc\n",
        "except Exception as e:\n",
        "    print(\"KMeans ROC Error:\", e)\n",
        "\n",
        "# MiniSom\n",
        "try:\n",
        "    som_params = study_som.best_trial.params\n",
        "    som = MiniSom(x=som_params['x'], y=som_params['y'], input_len=X_train_arr.shape[1],\n",
        "                  sigma=som_params['sigma'], learning_rate=som_params['learning_rate'], random_seed=42)\n",
        "    som.train(X_train_arr, 200)\n",
        "    def som_predict(som, X):\n",
        "        pred = []\n",
        "        for xx in X:\n",
        "            win = som.winner(xx)\n",
        "            cluster = win[1]\n",
        "            pred.append(cluster)\n",
        "        return np.array(pred)\n",
        "    som_pred = som_predict(som, X_test_arr)\n",
        "    som_label = cluster_to_label(y_test.values, som_pred)\n",
        "    auc = roc_auc_score(y_test, som_label)\n",
        "    fpr, tpr, _ = roc_curve(y_test, som_label)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"MiniSom (AUC={auc:.3f})\")  # 수정\n",
        "    roc_auc_dict[\"MiniSom_Optuna\"] = auc\n",
        "except Exception as e:\n",
        "    print(\"MiniSom ROC Error:\", e)\n",
        "\n",
        "# Keras 딥러닝 (MLP, LSTM, RNN)\n",
        "def fit_and_roc_keras(model_type, params, label):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    n1 = params['n1'] if 'n1' in params else 64\n",
        "    n2 = params['n2'] if 'n2' in params else 32\n",
        "    lr = params['lr'] if 'lr' in params else 1e-3\n",
        "    if model_type == 'MLP':\n",
        "        model.add(Dense(n1, activation='relu', input_shape=(X_train_arr.shape[1],)))\n",
        "        model.add(Dense(n2, activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    elif model_type == 'LSTM':\n",
        "        model.add(Reshape((X_train_arr.shape[1], 1), input_shape=(X_train_arr.shape[1],)))\n",
        "        model.add(LSTM(n1))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    elif model_type == 'RNN':\n",
        "        model.add(Reshape((X_train_arr.shape[1], 1), input_shape=(X_train_arr.shape[1],)))\n",
        "        model.add(SimpleRNN(n1))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='binary_crossentropy')\n",
        "    model.fit(X_train_arr, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "    y_score = model.predict(X_test_arr)\n",
        "    auc = roc_auc_score(y_test, y_score)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "    label_clean = label.replace(\"_\", \" \")\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"{label_clean} (AUC={auc:.3f})\")\n",
        "    roc_auc_dict[label] = auc\n",
        "\n",
        "fit_and_roc_keras('MLP', study_keras_mlp.best_trial.params, \"MLP Keras\")\n",
        "fit_and_roc_keras('LSTM', study_keras_lstm.best_trial.params, \"LSTM Keras\")\n",
        "fit_and_roc_keras('RNN', study_keras_rnn.best_trial.params, \"RNN Keras\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlabel('False Positive Rate', fontsize=13)\n",
        "plt.ylabel('True Positive Rate', fontsize=13)\n",
        "plt.title('모든 Optuna 튜닝 모델별 ROC Curve', fontsize=16)\n",
        "plt.legend(fontsize=9)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ztaR6tzhZnqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
